---
layout:     post
title:      "LLaMA1解析"
subtitle:   "模型架构、预训练、部署优化特点"
date:       2024-08-01 16:05:00
author:     "CS"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - LLaMA1
    - LLM
---

LLaMA 是Meta在2023年2月发布的一系列从 7B到 65B 参数的基础语言模型。LLaMA作为第一个向学术界开源的模型，在大模型爆发的时代具有标志性的意义。

为了更深入地理解LLaMA的技术特点，特地在此整理了LLaMA 1 模型架构、预训练、部署优化特点。话不多说，我们仔细看看吧。

![Image2](http://i-blog.csdnimg.cn/blog_migrate/4bebc488581ad27ca1af1831d052938a.jpeg)

### LLaMA简介

论文：[https://arxiv.org/abs/2302.13971](https://arxiv.org/abs/2302.13971)

Github：[https://github.com/facebookresearch/llama](https://github.com/facebookresearch/llama)

LLaMA-1提供了7B、13B、30B和65B四个参数量版本。Meta 训练这些模型使用了数万亿个 token，并且 **证明了完全可以只使用公开可得的数据集来训练最先进的模型，而无需使用专有和不可获取的数据集** 。特别是，LLaMA-13B 在大多数基准测试中表现优于GPT-3（175B），而 LLaMA-65B 在竞争中与最佳模型 Chinchilla70B 和PaLM-540B 持平。

Meta在训练这些模型时，也同时考虑到了模型在推理部署时的性能和要求 - 在大规模提供语言模型时，推理速度和推理性能变得至关重要。因此， **LLaMA选择用更小的模型，以及更多的数据集来进行预训练。**

（Hoffmann等人的最新工作显示， **在给定的计算预算下，最佳性能并不是由最大的模型实现的，而是由更多数据训练的较小模型实现的** 。

在这种情况下，考虑到推理运算以及目标性能水平， **首选模型不是训练速度最快的，而是推理速度最快的** 。虽然训练一个大型模型达到一定性能水平可能更便宜，但训练时间更长的较小模型在推理阶段最终会更经济。例如，虽然Hoffmann等人建议在 200B 标记上训练 10B 模型，但 Meta 发现 7B 模型的性能在 1T token 后仍在持续提高。）

### 具体方法

#### 预训练数据

LLaMA的预训练数据大约包含1.4T个token。其训练数据集是几个来源的混合，涵盖了不同的领域。

表1所示是 LLaMa 预训练数据的含量和分布：

表1：训练数据组成

<table><tbody>
<tr><td>数据集       </td><td>样本比例</td><td>Epochs</td><td>所占磁盘大小</td></tr>
<tr><td>CommonCrawl  </td><td>67.0%  </td><td>1.10</td><td>3.3 TB</td></tr>
<tr><td>C4           </td><td>15.0%  </td><td>1.06</td><td>783 GB</td></tr>
<tr><td>Github       </td><td>4.5%   </td><td>0.64</td><td>328 GB</td></tr>
<tr><td>Wikipedia    </td><td>4.5%   </td><td>2.45</td><td>83 GB</td></tr>
<tr><td>Books        </td><td>4.5%   </td><td>2.23</td><td>85 GB</td></tr>
<tr><td>ArXiv        </td><td>2.5%   </td><td>1.06</td><td>92 GB</td></tr>
<tr><td>StackExchange</td><td>2.0%   </td><td>1.03</td><td>78 GB</td></tr>
</tbody></table>

##### 1\.   **English CommonCrawl \[67%\]**

对五个 CommonCrawl 数据集进行预处理，时间跨度从2017年到2020年，使用 CCNet 来进行文本数据的预处理。

该过程先进行文本内容分片，然后进行段落归一化，并在此基础上在行级别进行数据去重；使用 fastText 线性分类器进行语言识别，以删除非英语页面；使用 n-gram 语言模型过滤低质量内容。

此外，还训练了一个线性模型，用于将页面分类为 Wikipedia 中的引用页面与随机抽样页面，并丢弃未被分类为引用的页面。（CCNet可参考[LLM Data Pipelines: 解析大语言模型训练数据集处理的复杂流程](https://juejin.cn/post/7259385807550087226)）

##### 2\.   **C4 \[15%\]**

C4也是属于Common Crawl数据集的一个经过粗略预处理的子集。在探索性实验中，研究团队观察到使用不同的预处理CommonCrawl数据集可以提高性能。因此，在数据中包含了公开可用的C4数据集。对于C4的预处理与 CCNet 的主要区别在于质量过滤，对于C4的预处理主要依赖于标点符号的存在或网页中的词语和句子数量等启发式方法。

##### 3\.   **Github \[4.5%\]**

使用 Google BigQuery 上可用的公共 GitHub 数据集。此外，使用基于行长度或字母数字字符比例的启发式方法过滤低质量文件，并使用正则表达式删除了诸如header之类的内容。最后，对生成的数据集进行了文件级别的去重，使用完全匹配的方法。

##### 4\.   **Wikipedia \[4.5%\]**

添加了截至2022年6月至8月的 Wikipedia 数据，涵盖20种语言。预处理包括：去除超链接、评论和其他格式样板。

##### 5\.   **Gutenberg and Books3 \[4.5%\]**

添加了两个书籍类的数据集，分别是 Gutenberg 以及 ThePile (训练 LLM 的常用公开数据集) 中的 Book3 部分。预处理包括重复数据删除，删除内容重叠超过 90% 的书籍。

##### 6\.   **ArXiv \[2.5%\]**

处理了arXiv Latex文件，以添加学术文本到数据集中。预处理包括：移除第一节之前的所有内容，以及参考文献；移除了.tex文件中的注释，并且内联展开了用户编写的定义和宏，以增加论文之间的一致性。

##### 7\.   **Stack Exchange \[2%\]**

这是一个涵盖各种领域的高质量问题和答案网站，范围从计算机科学到化学（类似知乎）。研究团队从 28 个最大的网站保留数据，从文本中删除 HTML 标签并按分数对答案进行排序。

> 笔者NOTE：对于LLM的训练，数据的质量是基础。对于这部分感兴趣的小伙伴，可以仔细看下LLaMA训练时对于不同数据集的处理方式。

####  训练方法

使用AdamW优化器，AdamW是对Adam优化器的改进，可以更有效地处理权重衰减，从而提高训练的稳定性。

β1和β2参数的选择影响训练过程的收敛行为和稳定性。

[余弦学习率](../csdn_md/2024-07-31-lr-decay-pytorch.md#4-cosine-learning-rate-decay)调度是一种有效的技术，用于在训练期间调整学习率，通过逐渐减少学习率，在某些情况下可以导致更好的收敛。

实施0.1的[权重衰减](https://blog.csdn.net/zhaohongfei_358/article/details/129625803)和1.0的梯度裁剪是预防过拟合和防止梯度爆炸的标准做法。

权重衰减：
``` python
optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.1)
```

梯度裁剪：
``` python
loss.backward() # 反向传播，计算梯度
# 梯度裁剪
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # max_norm 是裁剪的阈值
optimizer.step() # 更新参数
```

使用预热步骤（warm-up）是一种策略性方法，旨在训练过程初期稳定训练动态。根据模型大小调整学习率和批量大小是一种优化资源分配和效率的实用方法，有可能提高模型性能。
> 由于刚开始训练时,模型的权重(weights)是随机初始化的，此时若选择一个较大的学习率,可能带来模型的不稳定(振荡)，选择Warmup预热学习率的方式，可以使得开始训练的几个epoches或者一些steps内学习率较小,在预热的小学习率下，模型可以慢慢趋于稳定,等模型相对稳定后再选择预先设置的学习率进行训练,使得模型收敛速度变得更快，模型效果更佳。

 - `constant warmup`: 例如先用0.01的学习率训练直到训练误差低于80%(大概训练了400个steps)，然后使用0.1的学习率进行训练。
 - `gradual warmup`: 即从最初的小学习率开始，每个step增大一点点，直到达到最初设置的比较大的学习率时，采用最初设置的学习率进行训练。
 - [动态调整学习率](../csdn_md/2024-07-31-lr-decay-pytorch.md): 1.等间隔调整学习率(StepLR); 2.按需调整学习率(MultiStepLR); 3.指数衰减调整学习率(ExponentialLR); 4.余弦衰减调整学习率(CosineAnnealingLR); 5.自适应调整学习率(ReduceLROnPlateau); 6.自定义调整学习率(LambdaLR).

__gradual warmup:__
``` python
import torch
import torch.nn as nn
import torch.optim as optim
# 定义模型和优化器
model = YourModel()
optimizer = optim.Adam(model.parameters(), lr=0.001)
# 定义预热阶段的学习率逐步增加策略
warmup_steps = 1000  # 预热步数
initial_lr = 0.0001  # 初始学习率
final_lr = 0.001  # 最终学习率
# 定义一个线性增加学习率的函数
def warmup_learning_rate(optimizer, warmup_step, initial_lr, final_lr):
    if warmup_step == 0:
        return
    increment = (final_lr - initial_lr) / warmup_step
    for param_group in optimizer.param_groups:
        param_group['lr'] += increment
# 模型训练
for epoch in range(num_epochs):
    for batch_idx, (data, target) in enumerate(train_loader):
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        # 在预热阶段逐步增加学习率
        if epoch == 0 and batch_idx < warmup_steps:
            warmup_learning_rate(optimizer, warmup_steps, initial_lr, final_lr)
```

####  Tokenizer

使用字节对编码（BPE）算法对数据进行分词，使用 SentencePiece 的实现。值得注意的是，作者 **将所有数字分割成单个数字** 。

对于BPE的详细解释，可参考[BPE 算法原理及使用指南【深入浅出】](https://zhuanlan.zhihu.com/p/448147465)

常用tokenizer可以查看[LLM-tokenizers](/_posts/github_md/2024-08-02-LLM-tokenizers.md)

#### 模型架构

LLaMa 的网络还是主要基于 Transformer 架构。研究团队根据不同模型（如PaLM）的改进，从而利用了这些改进，来进一步提高LLaMA的训练稳定性、上下文长度性能。

以下是与原始架构的主要区别，以及从哪里得到了这种变化的灵感（括号中）。

1\.  **Pre-normalization \[受 GPT3 的启发\]** ：为了提高训练稳定性，LLaMa 对每个 Transformer 子层的输入进行归一化，而不是对输出进行归一化。LLaMa 使用了 RMSNorm 归一化函数。

[pytorch RMSNorm 示例](https://pytorch.org/docs/stable/generated/torch.nn.modules.normalization.RMSNorm.html)：
``` python 
# torch.nn.modules.normalization.RMSNorm(normalized_shape, eps=None, elementwise_affine=True, device=None, dtype=None)
rms_norm = nn.RMSNorm([2, 3])  # 归一化输入的维度
input = torch.randn(2, 2, 3)
rms_norm(input)
```
    

（关于Pre-norm vs Post-norm，可参考[为什么Pre Norm的效果不如Post Norm？](https://kexue.fm/archives/9009)）

2\. **SwiGLU 激活函数 \[受 PaLM 的启发\]：** LLaMa 使用 SwiGLU 激活函数替换 ReLU 以提高性能。SwiGLU是一种激活函数，它是GLU的一种变体，是谷歌2020年提出的一种激活函数，它可以提高transformer模型的性能。SwiGLU的优点是它可以动态地调整信息流的门控程度，根据输入的不同而变化，而且SwiGLU比ReLU更平滑，可以带来更好的优化和更快的收敛。

（关于SwiGLU激活函数，可参考[激活函数总结（八）：基于Gate mechanism机制的激活函数补充](https://blog.csdn.net/qq_36758270/article/details/132174106)）

SwiGLU实现代码如下：
``` python
import torch.nn as nn
import torch 
class SiLU(nn.Module): 
    def __init__(self):
        super().__init__()
    def forward(self, x): 
        return x * torch.sigmoid(x) 

class SwiGLU(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        self.silu = SiLU()
        self.expand_linear = nn.Linear(hidden_dim, 2 * hidden_dim)
    def forward(self, x):
        x = self.expand_linear(x)
        x, gate = x.chunk(2, dim=-1) # 在最后一个维度(横向)上切分出两个数组
        return x * self.silu(gate)  
```


3\. **Rotary Embeddings \[受 GPTNeo 的启发\]：** LLaMa 没有使用之前的绝对位置编码，而是使用了旋转位置编码（RoPE），可以提升模型的外推性。它的基本思想是通过一个旋转矩阵来调整每个单词或标记的嵌入向量，使得它们的内积只与它们的相对位置有关。旋转嵌入不需要预先定义或学习位置嵌入向量，而是在网络的每一层动态地添加位置信息。旋转嵌入有一些优点，比如可以处理任意长度的序列，可以提高模型的泛化能力，可以减少计算量，可以适用于线性Attention等。

（关于 RoPE 的具体细节，可参考[十分钟读懂旋转编码（RoPE）](https://zhuanlan.zhihu.com/p/647109286)）

> 笔者NOTE：LLM的架构是实现LLM基础性能的基石，对于这部分，各位小伙伴还是需要深入地了解一下各种架构的原理，以及其优劣势。

#### 优化器

LLaMA使用了AdamW优化器进行训练，优化器的超参数为 β1=0.9, β2=0.95

（关于AdamW这个大模型训练的优化器，可参考[当前训练神经网络最快的方式：AdamW优化算法+超级收敛-机器之心](https://www.jiqizhixin.com/articles/2018-07-03-14)）

下表为LLaMA不同参数大小模型的具体设置：

表2: LLaMA不同参数大小模型的具体设置

<table><tbody>
<tr><td>参数</td><td>维度（dim）</td><td>head个数</td><td>layer层数</td><td>学习率</td><td>batch size</td><td>token数量</td></tr>
<tr><td>6.7B</td><td>4096</td><td>32</td><td>32</td><td>3.0e−4</td><td>4M</td><td>1.0T</td></tr>
<tr><td>13.0B</td><td>5120</td><td>40</td><td>40</td><td>3.0e−4</td><td>4M</td><td>1.0T</td></tr>
<tr><td>32.5B</td><td>6656</td><td>52</td><td>60</td><td>1.5e−4</td><td>4M</td><td>1.4T</td></tr>
<tr><td>65.2B</td><td>8192</td><td>64</td><td>80</td><td>1.5e−4</td><td>4M</td><td>1.4T</td></tr>
</tbody></table>

#### 训练结果

如下图所示，7B、13B、33B和65模型的训练损失均呈下降趋势，且在所有token上训练完后，loss仍没有收敛的趋势。因此，在此时，增加训练的token数量，仍然可以使模型继续学习。

（LLaMA2就是在此结论的基础上，使用了更多的token进行训练）

![Image3](http://i-blog.csdnimg.cn/blog_migrate/88122c9920c052801a849f94d0f35419.png)

#### 高效部署

研究团队做了一些优化来提高模型的训练速度：

1.  **因果多头注意的有效实现：**使用因果多头注意的有效实现来减少内存使用和运行时间。该实现可在xformers库中获得，其灵感来自于固定激活值显存优化和FlashAttention。这是通过不存储注意力权重和不计算由于语言建模任务的因果性质而被掩盖的key/query分数来实现的。
    
2.  **激活重计算：**为了进一步提高训练效率，通过检查点减少了在向后传递过程中重新计算的激活量。更准确地说，节省了计算成本高的激活，比如线性层的输出。这是通过手动实现transformer层的backward函数来实现的，而不是依赖于PyTorch的autograd。
    
3.  **模型并行和序列并行：**为了从这种优化中充分受益，需要通过使用模型和序列并行来减少模型的内存使用。此外，还尽可能地重叠激活的计算和gpu之间通过网络的通信。
    

> 笔者NOTE：LLM的高效训练是LLM工程实现的基础，对于这部分，各位小伙伴还是需要深入地了解一下各种并行策略、因果多头注意的有效实现、 激活重计算、混合精度训练。

### 基于LLaMA的衍生模型（概述）

> 笔者NOTE：由于篇幅太长，因此在这篇里仅进行基于LLaMA的衍生模型的概述，之后也会出详细介绍各个衍生模型的文章

#### Alpaca

Alpaca是斯坦福在LLaMa-7B的基础上监督微调出来的模型，斯坦福是用OpenAI的Text-davinci-003 API配合self-instruct技术，使用175个提示语种子自动生成了52K条提示-回复的指示数据集，在LLaMa-7B上微调得到的模型，在8张80G的A100上训练了3小时。

可以说是以极低的成本生成了高质量的指令数据，并进行了指令微调，最终可以达到媲美GPT3.5的水平。

![Image 4: 52a1759645d77ec6abd99be1de544209.png](http://i-blog.csdnimg.cn/blog_migrate/24fb3f09d6cf6c5d53a6c662d91b2510.png)

#### **Vicuna**

Vicuna是在LLaMa-13B的基础上使用监督数据微调得到的模型，数据集来自于[http://ShareGPT.com](https://ShareGPT.com) 产生的用户对话数据，共70K条。使用Pytorch FSDP在8张A100上训练了一天。相较于Alpaca，Vicuna在训练中将序列长度由512扩展到了2048，并且通过梯度检测和flash attention来解决内存问题；调整训练损失考虑多轮对话，并仅根据模型的输出进行微调。通过GPT4来打分评测，Vicuna可以达到ChatGPT 90%的效果。并且还提供了可调用的分布式聊天服务[FastChat](https://github.com/lm-sys/FastChat)。

![Image 5: 23887e024123b01458b2a2b63d208ca1.png](http://i-blog.csdnimg.cn/blog_migrate/9daf21ce1e25d2c255570c07a7021616.png)
